# Projeto RAG Corporativo com Ollama e LangChain

Este projeto implementa um sistema de **RAG (Retrieval-Augmented Generation)** robusto e modular, capaz de responder perguntas com base em documentos internos. Ele utiliza **Ollama** para execu√ß√£o local de LLMs (Large Language Models) e **LangChain** para orquestra√ß√£o.

## üöÄ Funcionalidades

*   **Ingest√£o de Documentos**: Processamento autom√°tico de arquivos `.txt` da pasta `data/`.
*   **Busca Sem√¢ntica**: Utiliza **FAISS** como banco vetorial e embeddings `nomic-embed-text`.
*   **LLM Local**: Respostas geradas pelo modelo **Gemma 3 (4b)** rodando via Ollama.
*   **API REST (FastAPI)**: Interface de alta performance com endpoints para consulta e recarga de √≠ndice.
*   **Cache Inteligente**: Sistema de cache com TTL de 1 hora para respostas instant√¢neas a perguntas repetidas.
*   **Arquitetura Modular**: C√≥digo organizado em servi√ßos (`RAGService`, `IngestionService`) e configura√ß√£o centralizada.
*   **Monitoramento**: Logs detalhados com rota√ß√£o de arquivos e m√©tricas de tempo de execu√ß√£o.

## üõ†Ô∏è Arquitetura

O projeto segue uma arquitetura modular em camadas:

*   **`src/config.py`**: Configura√ß√µes globais (modelos, caminhos, par√¢metros).
*   **`src/rag_engine.py`**: Motor principal. Implementa Singleton para manter o modelo em mem√≥ria e gerencia o fluxo RAG (Retriever -> Prompt -> LLM). Inclui camada de cache.
*   **`src/ingestor.py`**: Servi√ßo respons√°vel por ler documentos, dividir em chunks e criar o √≠ndice vetorial.
*   **`src/logger.py`**: Sistema de logs centralizado.
*   **`api.py`**: Servidor FastAPI que exp√µe o `RAGService`.
*   **`main.py`**: Interface de linha de comando (CLI) para testes e ingest√£o.
*   **`crawler.py`**: Ferramenta auxiliar para baixar conte√∫do de sites.

## üì¶ Instala√ß√£o

### Pr√©-requisitos
*   Python 3.10+
*   [Ollama](https://ollama.com/) instalado e rodando.

### 1. Configurar Ambiente

Crie um ambiente virtual e instale as depend√™ncias:

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

pip install -r requirements.txt
```

### 2. Configurar Modelos Ollama

Certifique-se de que o Ollama est√° rodando (`ollama serve`) e baixe os modelos necess√°rios:

```bash
ollama pull nomic-embed-text  # Para embeddings (r√°pido e leve)
ollama pull gemma3:4b         # LLM para gera√ß√£o de texto
```

## üèÉ‚Äç‚ôÇÔ∏è Como Usar

### Op√ß√£o 1: API REST (Recomendado para Produ√ß√£o)

A API mant√©m o modelo carregado na mem√≥ria, garantindo respostas r√°pidas.

1.  **Inicie o servidor:**
    ```bash
    uvicorn api:app --reload
    ```
    *O servidor iniciar√° em `http://localhost:8000`.*

2.  **Fa√ßa perguntas (Exemplo com cURL):**
    ```bash
    curl -X POST "http://127.0.0.1:8000/query" \
         -H "Content-Type: application/json" \
         -d '{"question": "Quais servi√ßos a empresa oferece?"}'
    ```

3.  **Recarregar √çndice (ap√≥s adicionar novos arquivos):**
    ```bash
    curl -X POST "http://127.0.0.1:8000/reload"
    ```

### Op√ß√£o 2: Linha de Comando (CLI)

√ötil para testes r√°pidos ou scripts de automa√ß√£o.

*   **Ingest√£o de Dados** (Processar arquivos da pasta `data/`):
    ```bash
    python main.py ingest
    ```

*   **Chat Interativo**:
    ```bash
    python main.py chat
    ```

*   **Pergunta √önica**:
    ```bash
    python main.py query "Qual √© a vis√£o da empresa?"
    ```

### Op√ß√£o 3: Crawler (Coleta de Dados)

Para baixar conte√∫do de um site e salvar na pasta `data/`:

```bash
python crawler.py --url "https://exemplo.com.br" --depth 2
```

## üìä Logs e Monitoramento

Os logs s√£o salvos automaticamente na pasta `logs/` e tamb√©m exibidos no console.
*   **Arquivo**: `logs/app.log` (Rotacionado automaticamente, m√°x 5 arquivos de 5MB).
*   **Conte√∫do**: Detalhes de inicializa√ß√£o, tempo de resposta de cada etapa, erros e status de cache.

## ‚öôÔ∏è Personaliza√ß√£o

Voc√™ pode ajustar par√¢metros no arquivo `src/config.py`:
*   `CHUNK_SIZE`: Tamanho dos peda√ßos de texto.
*   `RETRIEVER_K`: Quantidade de trechos de contexto recuperados.
*   `CACHE_TTL`: Tempo de vida do cache (padr√£o: 3600s).
*   `LLM_MODEL`: Modelo Ollama a ser utilizado.
